# Elastic Kubernetes Service (EKS)
Is a managed container orchestration service that facilitates deploying, managing, and scaling Kubernetes applications in the AWS Cloud or on premises.

Helps to provide highly available and secure clusters. 

Also helps you automate key tasks such as patching, node provisioning, and updates.

### The need for container orchestration
Containers provide a standard way to package your application's code, configurations, and dependencies into a single object.

They are lightweight and provide a consistent, portable software environment for applications to run and scale anywhere.
Applications running in containers require a container orchestration platform to manage and scale deployments.

Different management platforms might manage different aspects of the container system.

# Benefits
### Managed Kubernetes service
![[Pasted image 20251126172417.png]]
### Tightly integrated with AWS services
![[Pasted image 20251126172440.png]]
### Built with the Community
![[Pasted image 20251126172458.png]]
### Conformant and Compatible
![[Pasted image 20251126172521.png]]

![[Pasted image 20251126173238.png]]
![[Pasted image 20251126173255.png]]

# Kubernetes
## Cluster
A set of worker machines, called nodes, that run containerized applications.
- Every cluster has at least one worker node. 
- A cluster also has a control plane that runs services that manage the cluster.
![[Pasted image 20251126174212.png]]

## Node
A virtual or physical machine, which runs the workload by grouping containers into pods and assigning those pods to run on nodes.
Each node is managed by the control plane and contains the services necessary to run pods.

## Pod
A group of one or more containers.
- Are defined by a PodSpec file, a specification for how to run the containers.
- Pods are the basic building block within Kubernetes for deployment, scaling, and replication.

![[Pasted image 20251126174315.png]]

## Ephemeral volume
Applications in a pod have access to shared volumes to facilitate data sharing in the pod and persistence of data across container restarts.
- When a pod ceases to exist, Kubernetes destroys ephemeral volumes.
## Persistent volume
Similar to an ephemeral volume but has a lifecycle independent of any individual pod that uses them.
Persistent volumes are backed by storage subsystems independent of cluster nodes.

## Service
A logical collection of pods and a means to access them.
The service is continually updated with the set of pods available, eliminating the need for pods to track other pods.
![[Pasted image 20251126175154.png|700]]
## Namespace
A virtual cluster that is backed by the same physical cluster.
- Physical clusters can have resources with the same name as long as they are in different namespaces.
- Namespaces are especially useful when you have multiple teams or projects using the same cluster.
![[Pasted image 20251126175146.png|700]]

## ReplicaSet
Ensures that a specific number of pod replicas are running at any given time.
![[Pasted image 20251126175130.png|700]]

## Deployment
Owns and manages ReplicaSets or individual pods.
- Describes a desired state in the deployment.
- The deployment then changes the actual state of the cluster to the desired state at a controlled rate.
![[Pasted image 20251126183351.png|700]]
## ConfigMap
Is an API object that stores nonconfidential data as key-value pairs used by other Kubernetes objects, such as pods.
- Use ConfigMaps to follow the best practice of portability by separating your configuration data from your application code.
![[Pasted image 20251126183413.png|700]]
## Secrets
All confidential data, such as AWS credentials.
Secrets restrict access to sensitive information. 
- Optionally, encryption can be turned on to improve security.

# Pod scheduling
Checks the resources required by the pods and uses that information to influence the scheduling decision.
The scheduler runs a series of filters to exclude ineligible nodes for pod placement.
![[Pasted image 20251126183538.png|700]]
# Control & Data plane
### Control Plane nodes
Manage the worker nodes and the pods in the cluster.
Determines when tasks are schedules and where they are routed to. It includes:
- Controller manager
- Cloud controller
- Scheduler
- API Server
- etcd
#### Controller manager
Runs background threads called controllers that detect and responder to cluster events
#### Cloud controller
Is a specific controller that interacts with the underlying cloud provider
#### Scheduler
Selects nodes for newly created containers to run on
#### API Server
Exposes the Kubernetes API and is the frontend for the Kubernetes control plane.
- Handles all communication from the cluster to the control plane.
- None of the other control plane components are designes to expose remote services
- The Kubernetes API is designes to scale horizontally, deploying more instances as necessary
#### etcd
The core persistence layer for Kubernetes.
- A highly available distributed key value store
- This is where the critical cluster data and state are stored
### Data plane
Worker nodes host the pods that are the components of the application workload.
Is where the tasks are run. This is all done on the worker nodes. It includes:
- Worker nodes
- kube-proxy
- Contarimer runtime
- kubelet
- Pods
#### kube-proxy
Helps with networking. Maintains network rules on the host and performs any connection forwarding that may be necessary

#### Container runtime
Kubernetes supports several runtimes, with Docker being the most common
#### kubelet
The primaty agent that runs on the worker nodes. 
- Makes sure that the right containers are running in a pod and that tey are healthy
#### Pods
A group of one or more containers
- The containers in a pod are always colocated, scheduled and managed together, cannot split containers in a pod across nodes
- Applications in a pod can easily communicate with each other
- Considered to be relatively ephemeral entities
	- Can disappear if the become unhealthy, and new ones can take their place
## Custom resources
In addition to the resources that Kubernetes defines (such as pods and deployments), can also extend the Kubernetes API and create custom resources.
- Custom resources are created with a Custom Resource Definition (CRD).
- Custom resources can be controlled with custom controllers. Custom controllers run in pods on the worker nodes of your cluster.
- When used to automate the management of custom resources in a cluster, custom controllers are referred to as operators
- It is good practice to use operators instead of manually updating your native Kubernetes objects.
## kubectl
Is a command line interface (CLI) for communicating with the Kubernetes API server.
- It provides commands to create resources, view detailed information about the cluster and resources, and access troubleshooting tools.
- kubectl commands are used to roll out, scale, and automatically scale resources.
- Syntax: **`kubectl [command] [TYPE] [NAME] [flags]`** 
	- *Command*: Specifies the operation you are performing.
	- _Type_: Specifies the resource type.
	- _Name_: Specifies the name of the resource.
	- _Flag_: Specifies optional flags.
# Amazon EKS control plane
Amazon EKS provides a scalable, highly available control plane. 

It automatically manages the availability and scalability of the Kubernetes API servers and the etcd persistence layer for each cluster.
### Amazon EKS availability and API
Amazon EKS control plane consists of at least two API server nodes and three etcd nodes across three Availability Zones.

It automatically detects and replaces unhealthy control plane nodes, which removes a significant operational burden for running Kubernetes. With this capability, you can focus on building your applications instead of managing AWS infrastructure.

To get started with Amazon EKS, you provision your cluster of worker nodes. Amazon EKS handles the provisioning, scaling, and management of the Kubernetes control plane in a highly available and secure configuration.
Then connect to the Amazon EKS cluster using the graphical or command line interface.
After that, it's ready to deploy the Kubernetes applications to the Amazon EKS cluster.

Amazon EKS manages the Kubernetes control plane with the Amazon EKS API.
Can use one of two CLIs to interact with the Amazon EKS API: **Amazon EKS CLI or eksctl**
The eksctl utility uses AWS CloudFormation in the background to build clusters based on the options you specify.

## Amazon EKS API
![[Pasted image 20251126191301.png]]
## Kubernetes API
![[Pasted image 20251126191325.png]]
# Amazon EKS data plane
## Self-managed Nodes
Only the control plane is managed by Amazon EKS. You completely control and manage your data plane nodes (including provisioning, updating, monitoring, and other tasks).

## Managed Node groups
Managed node groups use the Amazon EKS API to start and manage the Amazon Elastic Compute Cloud (Amazon EC2) instances that run containers for an Amazon EKS cluster.
![[Pasted image 20251126192024.png]]

![[Pasted image 20251126192048.png]]

# Cluster
A deploy of the Amazon EKS managed control plane. Adding worker nodes to your cluster is a separate step
### Interfaces for cluster
- **eksctl utility**: Command line tool that simpifies cluster reaction. Using this method, can create a cluster with a single command
- **AWS Management Console**: Provides a graphical interface that handles many of the complexities of cluster creation for me. Have to perform some steps using the AWS CLI when using this method
- **AWS CLI**: Offers the most potential for costomization. It also has the most complexity of the three
### Creating a cluster with eksctl
By default, eksctl automates many of the steps involved in cluster and worker node creation. **Overview**:
1. Creates IAM roles for the cluster and worker nodes
2. Creates a dedicated VPC with Classless Inter-Domain Routing (CIDR) 192.168.0.0/16
3. Creates a cluster and a nodegroup
4. Configures access to API endpoints
5. Installs CoreDNS
6. Writes a kubeconfig file for the cluster

- **eksctl create cluster** crea y levanta el cluster
- **kubectl get nodes** muestra los nodos del cluster
	- Puedo crear un config file para cada eksctl (YAML) o dejar uno por default/sin
- **eksctl**Â translates the instructions in your configuration file to equivalent CloudFormation templates

# Examine an Amazon EKS cluster in the AWS Management Console
### Horizontal scaling
- Can increase or decrease capacity by adding/removing compute resources
![[Pasted image 20251206120440.png]]

### Vertical scaling
- Increases performance by adding more resources to the compute resource, such as faster (or more) CPUs, memory, storage, etc
![[Pasted image 20251206120524.png]]

## Kubernetes automatic scaling
![[Pasted image 20251206120606.png]]

## Cluster Autoscaler
- Automatically adjusts the number of nodes in the cluster when pods fail to launch
- The failure can result from a lack of resources or when nodes in the cluster are underutilized and their pods can be rescheduled onto other nodes in the cluster
- In EKS, this is accomplished by adding the worker nodes to EC2 Auto Scaling groups, by deploying a cluster with managed node groups using **eksctl**

![[Pasted image 20251206121934.png]]
- EC2 Auto Scaling automatically adjusts capacity to maintain steady, predictable performance at the lowest possible cost. 
- The dynamic scaling capabilities of EC2 automatically increase/decrease capacity based on load or other metrics

![[Pasted image 20251206122053.png]]
- When demand goes up, EC2 Auto Scaling scales out

![[Pasted image 20251206122120.png]]
- When demand goes down, EC2 Auto Scaling scales in. The number of instances will not go above the maximum or below the minimum

## Karpenter (Alternative to Cluster Autoscaler)  
- Is a node lifecycle management solution
- Observes incoming pods and launches the right instances for the situation
- Instance selection decisions are intent-based and driven by the specification of incoming pods
	- Including resource requests and scheduling constraints
- **When deployed**:
	- Launch nodes for unscheduled pods
	- Replace existing nodes to improve resource utilization
	- Terminate nodes if outdated or no longer needed
	- Drain nodes gracefully before preemption

## Horizontal Pod Autoscaler (HPA)
- Kubernetes component that automatically scales the service in/out 
- Based on CPU utilization or other metrics defined through the Kubernetes metrics server

![[Pasted image 20251206194530.png]]
- The Horizontal Pod Autoscaler uses CPU utilization by default but can be configured to use custom metrics, such as metrics from Amazon CloudWatch

## Vertical Pod Autoscaler (VPA)
- Automatically adjusts the CPU and memory reservations for the pods to help rightsize the applications
- This adjustment can improve cluster resource utilization and free up CPU and memory for other pods
#### VPA example
![[Pasted image 20251206194923.png]]
![[Pasted image 20251206194944.png]]
![[Pasted image 20251206194959.png]]
![[Pasted image 20251206195010.png]]

## Overview of Amazon EKS communication  
- To simplify inter-node comm, Amazon EKS integrates Amazon VPS networking into Kubernetes 
	- Through the Amazon VPC Container Network Interface (CNI) plugin for Kubernetes
- The Amazon VPC CNI plugin allows Kubernetes pods to have the same IP address inside the pod as they to on the Amazon VPC network
![[Pasted image 20251206201403.png]]
- Amazon VPC as a virtual data center
	- Isolated section of the AWS Cloud where you can launch AWS resources (like Amazon EC2 instances), in a virtual network defined
- An amazon VPC is created in a single AWS Region, but it spans all of the Availability Zones within that Region
- When you create a VPS, you must specify a range of IPv4 addresses for the VPS as a CIDR block.
	- This is the primary CIDR for your VPC
- After creating it, can add 1+ subnets in each Availability Zone
- When you create a subnet, you specify the CIDR block for the subnet, which is a subnet of the VPC CIDR block
- Each subnet must reside entirely in one Avalibaility Zone and cannot span zones

### Types of communication in Amazon EKS
- Interpod comm between containers
- Comm between pods on the same node or pods on different nodes
- Ingress connections from outside the cluster
	- In some cases, the default Kubernetes methods are used
	- In other, specifically inter-node comm and ingress methos specific to Amazon EKS
- #### 1. Intrapod communication
	- Containers in a pod share a Linux namespace and can communicate with each other using localhost
	- In Kubernetes networking, the IP address with which a container identifies is the same IP address for all entities in the network
	- All containers can comm with all other containers in a pod without NAT
- #### 2. Intrahost communication
	- The host node also has a Linux namespace
	- Each namespace has its own routing table
	- The pod namespace and host namespace are connected by a Linux virtual Ethernet (veth) device
	- A pair of veths creates a tunnel between the default host namespace and the pod namespace
	- Pod-to-Pod comm in the host happens through this veth tunnel
	- Each node is allocated a network range for containers and each pod gets an IP address in that range, allowing containers on the same host to comm
- #### 3. Interhost communication
	- To simplify internode communication, EKS integrates VPC networking into Kubernetes through the VPC CNI plugin for Kubernetes
	- CNI allows Kubernetes pods to have the same IP address inside the pod as they do on the VPC network
	- ![[Pasted image 20251207130037.png]]

## Review: Kubernetes services
- The native service objects in Kubernetes solve the issue of pods disappearing and new pods being created with different IP addresses
	- Instead of trying to communicate to the IP address of ephemeral pods, communicate to the IP address of the service
	- The service is continually updated with the pod statuses and directs to a healthy pod
- A service object provides a constant IP address and communication port as an entry point to a group of pods
	- Each service object has an IP address and port that does not change for as long as the service exists
	- Internal or external clients can reach your application running in a group of pods by connecting to the service IP address and port
	- Those connections are then routed to one of the pods backing that service
![[Pasted image 20251207131105.png]]
![[Pasted image 20251207133212.png]]
## Ingress
- WIth Kubernetes ingress objects, it can reduce the number of load balancers you use
- An ingress object exposes HTTP and HTTPS routes from outside the cluster to your services and defines traffic rules
- ![[Pasted image 20251207133453.png]]
## AWS Load Balancer Controller
- A controller that manages Elastic Load Balancing (ELB) for a Kubernetes cluster
- The **load balancers** can be 
	- *App Load Balancers* when you create a Kubernetes Ingress
	- *Network Load Balancers* when you create a Kubernetes service of type LoadBalancer
- *App Load Balancers* balances app traffic at Layer 7 of the OSI model
- *Network Load Balancers* balances network traffic at Layer 4
- *App Load Balancers* can be used with pods that are deployed to nodes or to Fargate. Can be deployed to public or private subnets
- *Network Load Balancers* can load balance network traffic to pods deployed to Amazon EC2 IP

# Managing storage in Amazon EKS
Provides the benefit of using other AWS services including several storage services

## Kubernetes persistent storage
- Requires at least two Kubernetes objects, a persistent volume (PV) and a persistent volume claim (PVC)
- A PV is similar to ephemeral volumes but has a lifecycle independent of any individual pod
- A PVC is a request for storage by a cluster user, which means the request includes details about storage
